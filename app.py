import streamlit as st
import os
import tempfile
from dotenv import load_dotenv
from rag_index_builder import build_index_from_pdf
from tools import retrieve_legal_context
from autogen import AssistantAgent, UserProxyAgent

# Load environment variables
load_dotenv()

AZURE_OPENAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION")

# LLM config
llm_config = {
    "config_list": [
        {
            "api_key": AZURE_OPENAI_API_KEY,
            "base_url": AZURE_OPENAI_ENDPOINT,
            "api_type": "azure",
            "api_version": AZURE_OPENAI_API_VERSION,
            "model": "gpt-4",  # Replace with your Azure deployment name
        }
    ],
    "temperature": 0
}

# Define termination check
def is_termination_msg(msg):
    return msg.get("content") and "TERMINATE" in msg["content"]

# Core chat function
def run_agent(query):
    legal_assistant = AssistantAgent(
        name="LegalAssistant",
        system_message=(
            "You are a helpful legal assistant that answers user queries ONLY by calling the 'retrieve_legal_context' tool. "
            "After answering, always respond with 'TERMINATE' to end the chat."
        ),
        llm_config=llm_config,
    )

    user = UserProxyAgent(
        name="User",
        llm_config=False,
        human_input_mode="NEVER",
        is_termination_msg=is_termination_msg,
        code_execution_config={"use_docker": False}
    )

    legal_assistant.register_for_llm(
        name="retrieve_legal_context",
        description="Retrieve relevant legal context from the indexed legal documents based on the query."
    )(retrieve_legal_context)

    user.register_for_execution(
        name="retrieve_legal_context"
    )(retrieve_legal_context)

    chat_result = user.initiate_chat(
        legal_assistant,
        message=query,
        summary_method="reflection_with_llm"
    )

    history = getattr(chat_result, "chat_history", None)
    if history is None:
        return "No chat history found.", []

    # Extract final message
    for msg in reversed(history):
        if msg.get("role") == "user" and msg.get("name") == "LegalAssistant":
            final_content = msg.get("content", "").replace("TERMINATE", "").strip()
            return final_content, history

    return "No valid answer was generated by the assistant.", history

# Streamlit app layout
st.set_page_config(page_title="üìÑ Legal RAG Assistant", layout="wide")
st.markdown("""
    <div style="
        background: linear-gradient(135deg, #e0f7fa, #ffdde1);
        padding: 25px 35px;
        border-radius: 15px;
        max-width: 850px;
        margin: 0 auto 30px auto;
        text-align: center;
        box-shadow: 0 8px 25px rgba(0, 0, 0, 0.1);
    ">
        <h1 style="
            color: #1a237e;
            font-size: 2.8em;
            margin-bottom: 12px;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        ">
            ‚öñÔ∏è Legal RAG Assistant
        </h1>
        <p style="
            color: #37474f;
            font-size: 1.15em;
            font-weight: 500;
            line-height: 1.6;
        ">
            Ask your legal questions with confidence üíº<br/>
            Powered by Retrieval-Augmented Generation (RAG) üß†üîç
        </p>
    </div>
""", unsafe_allow_html=True)



st.markdown("Upload your **legal documents** and ask your legal questions. Our AI assistant will respond with contextual answers extracted directly from your document. üîçüìò")

# File upload
uploaded_file = st.file_uploader("üìé Upload a legal PDF document", type=["pdf"])

if uploaded_file is not None:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.read())
        tmp_pdf_path = tmp_file.name

    st.success("‚úÖ PDF uploaded successfully!")

    with st.spinner("üîß Building FAISS index from uploaded PDF..."):
        build_index_from_pdf(tmp_pdf_path, persist_dir="rag_faiss_store")
    st.success("üìö Index built and ready to go!")

    # Query section
    st.markdown("---")
    st.subheader("üí¨ Ask Your Legal Question")
    query = st.text_input("Type your legal query here:")

    if query:
        with st.spinner("ü§ñ Retrieving answer from Legal Assistant..."):
            answer, chat_history = run_agent(query)

        st.markdown("### üß† Agent's Answer:")
        st.success(answer)

        # Optional: Expandable chat history
        with st.expander("üìú View Full Chat History"):
            for msg in chat_history:
                role = msg.get("role", "").capitalize()
                name = msg.get("name", "")
                content = msg.get("content", "")
                if content:
                    st.markdown(f"**{role} ({name})**: {content}")
else:
    st.info("üìÇ Please upload a legal PDF to begin.")
